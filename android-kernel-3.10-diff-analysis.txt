git log -p arch/x86/include/asm/idle.h
commit 302467bb15d4cb3486696bd4063a72237749c73f
Author: Todd Poynor <toddpoynor@google.com>
Date:   Wed Jun 15 17:21:57 2011 -0700

    Move x86_64 idle notifiers to generic
    
    Move the x86_64 idle notifiers originally by Andi Kleen and Venkatesh
    Pallipadi to generic.
    
    Change-Id: Idf29cda15be151f494ff245933c12462643388d5
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

diff --git a/arch/x86/include/asm/idle.h b/arch/x86/include/asm/idle.h
index c5d1785..02bab09 100644
--- a/arch/x86/include/asm/idle.h
+++ b/arch/x86/include/asm/idle.h
@@ -1,13 +1,6 @@
 #ifndef _ASM_X86_IDLE_H
 #define _ASM_X86_IDLE_H
 
-#define IDLE_START 1
-#define IDLE_END 2
-
-struct notifier_block;
-void idle_notifier_register(struct notifier_block *n);
-void idle_notifier_unregister(struct notifier_block *n);
-
 #ifdef CONFIG_X86_64
 void enter_idle(void);
 void exit_idle(void);


==========================================
 git log -p arch/x86/include/asm/barrier.h
commit a03a2426ea9f1d9dada33cf4a824f63e8f916c9d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Introduce smp_load_acquire(), smp_store_release()
    
    A number of situations currently require the heavyweight smp_mb(),
    even though there is no need to order prior stores against later
    loads.  Many architectures have much cheaper ways to handle these
    situations, but the Linux kernel currently has no portable way
    to make use of them.
    
    This commit therefore supplies smp_load_acquire() and
    smp_store_release() to remedy this situation.  The new
    smp_load_acquire() primitive orders the specified load against
    any subsequent reads or writes, while the new smp_store_release()
    primitive orders the specifed store against any prior reads or
    writes.  These primitives allow array-based circular FIFOs to be
    implemented without an smp_mb(), and also allow a theoretical
    hole in rcu_assign_pointer() to be closed at no additional
    expense on most architectures.
    
    In addition, the RCU experience transitioning from explicit
    smp_read_barrier_depends() and smp_wmb() to rcu_dereference()
    and rcu_assign_pointer(), respectively resulted in substantial
    improvements in readability.  It therefore seems likely that
    replacing other explicit barriers with smp_load_acquire() and
    smp_store_release() will provide similar benefits.  It appears
    that roughly half of the explicit barriers in core kernel code
    might be so replaced.
    
    [Changelog by PaulMck]
    (cherry picked from commit 47933ad41a86a4a9b50bed7c9b9bd2ba242aac63)
    
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131213150640.908486364@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h
index c6cd358..04a4890 100644
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@ -92,12 +92,53 @@
 #endif
 #define smp_read_barrier_depends()     read_barrier_depends()
 #define set_mb(var, value) do { (void)xchg(&var, value); } while (0)
-#else
+#else /* !SMP */
 #define smp_mb()       barrier()
 #define smp_rmb()      barrier()
 #define smp_wmb()      barrier()
 #define smp_read_barrier_depends()     do { } while (0)
 #define set_mb(var, value) do { var = value; barrier(); } while (0)
+#endif /* SMP */
+
+#if defined(CONFIG_X86_OOSTORE) || defined(CONFIG_X86_PPRO_FENCE)
+
+/*
+ * For either of these options x86 doesn't have a strong TSO memory
+ * model and we should fall back to full barriers.
+ */
+
+#define smp_store_release(p, v)                                                \
+do {                                                                   \
+       compiletime_assert_atomic_type(*p);                             \
+       smp_mb();                                                       \
+       ACCESS_ONCE(*p) = (v);                                          \
+} while (0)
+
+#define smp_load_acquire(p)                                            \
+({                                                                     \
+       typeof(*p) ___p1 = ACCESS_ONCE(*p);                             \
+       compiletime_assert_atomic_type(*p);                             \
+       smp_mb();                                                       \
+       ___p1;                                                          \
+})
+
+#else /* regular x86 TSO memory ordering */
+
+#define smp_store_release(p, v)                                                \
+do {                                                                   \
+       compiletime_assert_atomic_type(*p);                             \
+       barrier();                                                      \
+       ACCESS_ONCE(*p) = (v);                                          \
+} while (0)
+
+#define smp_load_acquire(p)                                            \
+({                                                                     \
+       typeof(*p) ___p1 = ACCESS_ONCE(*p);                             \
+       compiletime_assert_atomic_type(*p);                             \
+       barrier();                                                      \
+       ___p1;                                                          \
+})
+
 #endif
 
 /*
 
==========================================
 git log -p arch/x86/include/asm/processor.h
commit 9be223506831100e234f46103355310e479be9c0
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Sun Feb 1 21:35:38 2015 -0500

    bludgeon the flounder kernel until it builds on i386 for qemu testing
    
    Change-Id: Ib0a45f164301d18629fca0c89a1b17f7a435a8d4
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Theodore Ts'o <tytso@google.com>

diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 22224b3..57437a1 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -660,6 +660,10 @@ static inline void cpu_relax(void)
        rep_nop();
 }
 
+#ifndef cpu_read_relax
+#define cpu_read_relax() cpu_relax()
+#endif
+
 /* Stop speculative execution and prefetching of modified code. */
 static inline void sync_core(void)
 {



==============================================
 git log -p arch/x86/include/asm/syscall.h
commit 9499cd23f9d05ba159fac6d55dc35a7f49f9ce76
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Mar 11 12:48:43 2014 -0400

    syscall_get_arch: remove useless function arguments
    
    Every caller of syscall_get_arch() uses current for the task and no
    implementors of the function need args.  So just get rid of both of
    those things.  Admittedly, since these are inline functions we aren't
    wasting stack space, but it just makes the prototypes better.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-mips@linux-mips.org
    Cc: linux390@de.ibm.com
    Cc: x86@kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    
    Conflicts:
        arch/mips/include/asm/syscall.h
        arch/mips/kernel/ptrace.c

diff --git a/arch/x86/include/asm/syscall.h b/arch/x86/include/asm/syscall.h
index 2e188d6..f106908 100644
--- a/arch/x86/include/asm/syscall.h
+++ b/arch/x86/include/asm/syscall.h
@@ -90,8 +90,7 @@ static inline void syscall_set_arguments(struct task_struct *task,
        memcpy(&regs->bx + i, args, n * sizeof(args[0]));
 }
 
-static inline int syscall_get_arch(struct task_struct *task,
-                                  struct pt_regs *regs)
+static inline int syscall_get_arch(void)
 {
        return AUDIT_ARCH_I386;
 }
@@ -220,8 +219,7 @@ static inline void syscall_set_arguments(struct task_struct *task,
                }
 }
 
-static inline int syscall_get_arch(struct task_struct *task,
-                                  struct pt_regs *regs)
+static inline int syscall_get_arch(void)
 {
 #ifdef CONFIG_IA32_EMULATION
        /*
@@ -233,7 +231,7 @@ static inline int syscall_get_arch(struct task_struct *task,
         *
         * x32 tasks should be considered AUDIT_ARCH_X86_64.
         */
-       if (task_thread_info(task)->status & TS_COMPAT)
+       if (task_thread_info(current)->status & TS_COMPAT)
                return AUDIT_ARCH_I386;
 #endif
        /* Both x32 and x86_64 are considered "64-bit". */


==============================================
git log -p arch/x86/kernel/process.c
commit 302467bb15d4cb3486696bd4063a72237749c73f
Author: Todd Poynor <toddpoynor@google.com>
Date:   Wed Jun 15 17:21:57 2011 -0700

    Move x86_64 idle notifiers to generic
    
    Move the x86_64 idle notifiers originally by Andi Kleen and Venkatesh
    Pallipadi to generic.
    
    Change-Id: Idf29cda15be151f494ff245933c12462643388d5
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 81a5f5e..1ce8966 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -40,19 +40,6 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
-static ATOMIC_NOTIFIER_HEAD(idle_notifier);
-
-void idle_notifier_register(struct notifier_block *n)
-{
-       atomic_notifier_chain_register(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-       atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 #endif
 
 struct kmem_cache *task_xstate_cachep;
@@ -257,14 +244,14 @@ static inline void play_dead(void)
 void enter_idle(void)
 {
        this_cpu_write(is_idle, 1);
-       atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
+       idle_notifier_call_chain(IDLE_START);
 }
 
 static void __exit_idle(void)
 {
        if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
                return;
-       atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
+       idle_notifier_call_chain(IDLE_END);
 }
 
 /* Called from interrupts to signify idle end */
===========================================================
git log -p arch/x86/kernel/process.c
commit 302467bb15d4cb3486696bd4063a72237749c73f
Author: Todd Poynor <toddpoynor@google.com>
Date:   Wed Jun 15 17:21:57 2011 -0700

    Move x86_64 idle notifiers to generic
    
    Move the x86_64 idle notifiers originally by Andi Kleen and Venkatesh
    Pallipadi to generic.
    
    Change-Id: Idf29cda15be151f494ff245933c12462643388d5
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Todd Poynor <toddpoynor@google.com>

diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 81a5f5e..1ce8966 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -40,19 +40,6 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
-static ATOMIC_NOTIFIER_HEAD(idle_notifier);
-
-void idle_notifier_register(struct notifier_block *n)
-{
-       atomic_notifier_chain_register(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_register);
-
-void idle_notifier_unregister(struct notifier_block *n)
-{
-       atomic_notifier_chain_unregister(&idle_notifier, n);
-}
-EXPORT_SYMBOL_GPL(idle_notifier_unregister);
 #endif
 
 struct kmem_cache *task_xstate_cachep;
@@ -257,14 +244,14 @@ static inline void play_dead(void)
 void enter_idle(void)
 {
        this_cpu_write(is_idle, 1);
-       atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
+       idle_notifier_call_chain(IDLE_START);
 }
 
 static void __exit_idle(void)
 {
        if (x86_test_and_clear_bit_percpu(0, is_idle) == 0)
                return;
-       atomic_notifier_call_chain(&idle_notifier, IDLE_END, NULL);
+       idle_notifier_call_chain(IDLE_END);
 }
 
 /* Called from interrupts to signify idle end */

=====================================
 git log -p arch/x86/syscalls/syscall_32.tbl
commit 900e9fd0d5d15c596cacfb89ce007c933cea6e1c
Author: Lee Campbell <leecam@chromium.org>
Date:   Wed Oct 8 14:40:22 2014 -0700

    seccomp: fix syscall numbers for x86 and x86_64
    
    Correcting syscall numbers for seccomp
    
    Signed-off-by: Lee Campbell <leecam@chromium.org>

diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index 8605e9e..01ed502 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -357,4 +357,7 @@
 348    i386    process_vm_writev       sys_process_vm_writev           compat_sys_process_vm_writev
 349    i386    kcmp                    sys_kcmp
 350    i386    finit_module            sys_finit_module
-351    i386    seccomp                 sys_seccomp
+# 351  i386    sched_setattr           sys_sched_setattr
+# 352  i386    sched_getattr           sys_sched_getattr
+# 353  i386    renameat2               sys_renameat2
+354    i386    seccomp                 sys_seccomp

commit e985fd474debedb269fba27006eda50d0b6f07ef
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Jun 25 16:08:24 2014 -0700

    seccomp: add "seccomp" syscall
    
    This adds the new "seccomp" syscall with both an "operation" and "flags"
    parameter for future expansion. The third argument is a pointer value,
    used with the SECCOMP_SET_MODE_FILTER operation. Currently, flags must
    be 0. This is functionally equivalent to prctl(PR_SET_SECCOMP, ...).
    
    In addition to the TSYNC flag later in this patch series, there is a
    non-zero chance that this syscall could be used for configuring a fixed
    argument area for seccomp-tracer-aware processes to pass syscall arguments
    in the future. Hence, the use of "seccomp" not simply "seccomp_add_filter"
    for this syscall. Additionally, this syscall uses operation, flags,
    and user pointer for arguments because strictly passing arguments via
    a user pointer would mean seccomp itself would be unable to trivially
    filter the seccomp syscall itself.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@amacapital.net>
    
    Conflicts:
        arch/x86/syscalls/syscall_32.tbl
        arch/x86/syscalls/syscall_64.tbl
        include/uapi/asm-generic/unistd.h
        kernel/seccomp.c
    
    And fixup of unistd32.h to truly enable sys_secomp.
    
    Change-Id: I95bea02382c52007d22e5e9dc563c7d055c2c83f

diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index aabfb83..8605e9e 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -357,3 +357,4 @@
 348    i386    process_vm_writev       sys_process_vm_writev           compat_sys_process_vm_writev
 349    i386    kcmp                    sys_kcmp
 350    i386    finit_module            sys_finit_module
+351    i386    seccomp                 sys_seccomp
=====================================================
 git log -p arch/x86/um/signal.c
commit 0f708442d160846f3e0b0f390c0534e801f7b5f5
Author: Richard Weinberger <richard@nod.at>
Date:   Fri Jul 19 11:35:32 2013 +0200

    um: remove dead code
    
    "me" is not used.
    
    [cherry-pick of upstream 9e82d450531c79b18ab18c9b9645cdd9db31ee98]
    
    Change-Id: Ifc3550184931dddf8feebd6c3137e60b97f6a0f1
    Signed-off-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>

diff --git a/arch/x86/um/signal.c b/arch/x86/um/signal.c
index ae7319d..5e04a1c 100644
--- a/arch/x86/um/signal.c
+++ b/arch/x86/um/signal.c
@@ -508,7 +508,6 @@ int setup_signal_stack_si(unsigned long stack_top, int sig,
 {
        struct rt_sigframe __user *frame;
        int err = 0;
-       struct task_struct *me = current;
 
        frame = (struct rt_sigframe __user *)
                round_down(stack_top - sizeof(struct rt_sigframe), 16);

=====================================================
 git log -p block/genhd.c
commit ac0949ebe87beb9cc06a4f27ab6be1bd39c2fa3c
Author: San Mehat <san@google.com>
Date:   Sat Oct 10 09:35:24 2009 -0700

    block: genhd: Add disk/partition specific uevent callbacks for partition info
    
    For disk devices, a new uevent parameter 'NPARTS' specifies the number
    of partitions detected by the kernel. Partition devices get 'PARTN' which
    specifies the partitions index in the table, and 'PARTNAME', which
    specifies PARTNAME specifices the partition name of a partition device
    
    Signed-off-by: Dima Zavin <dima@android.com>

diff --git a/block/genhd.c b/block/genhd.c
index 20625ee..a534eaf 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -1107,6 +1107,22 @@ static void disk_release(struct device *dev)
                blk_put_queue(disk->queue);
        kfree(disk);
 }
+
+static int disk_uevent(struct device *dev, struct kobj_uevent_env *env)
+{
+       struct gendisk *disk = dev_to_disk(dev);
+       struct disk_part_iter piter;
+       struct hd_struct *part;
+       int cnt = 0;
+
+       disk_part_iter_init(&piter, disk, 0);
+       while((part = disk_part_iter_next(&piter)))
+               cnt++;
+       disk_part_iter_exit(&piter);
+       add_uevent_var(env, "NPARTS=%u", cnt);
+       return 0;
+}
+
 struct class block_class = {
        .name           = "block",
 };
@@ -1126,6 +1142,7 @@ static struct device_type disk_type = {
        .groups         = disk_attr_groups,
        .release        = disk_release,
        .devnode        = block_devnode,
+       .uevent         = disk_uevent,
 };
 
 #ifdef CONFIG_PROC_FS

=====================================
git log -p block/partition-generic.c
commit ac0949ebe87beb9cc06a4f27ab6be1bd39c2fa3c
Author: San Mehat <san@google.com>
Date:   Sat Oct 10 09:35:24 2009 -0700

    block: genhd: Add disk/partition specific uevent callbacks for partition info
    
    For disk devices, a new uevent parameter 'NPARTS' specifies the number
    of partitions detected by the kernel. Partition devices get 'PARTN' which
    specifies the partitions index in the table, and 'PARTNAME', which
    specifies PARTNAME specifices the partition name of a partition device
    
    Signed-off-by: Dima Zavin <dima@android.com>

diff --git a/block/partition-generic.c b/block/partition-generic.c
index 789cdea..c7942ac 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -216,10 +216,21 @@ static void part_release(struct device *dev)
        kfree(p);
 }
 
+static int part_uevent(struct device *dev, struct kobj_uevent_env *env)
+{
+       struct hd_struct *part = dev_to_part(dev);
+
+       add_uevent_var(env, "PARTN=%u", part->partno);
+       if (part->info && part->info->volname[0])
+               add_uevent_var(env, "PARTNAME=%s", part->info->volname);
+       return 0;
+}
+
 struct device_type part_type = {
        .name           = "partition",
        .groups         = part_attr_groups,
        .release        = part_release,
+       .uevent         = part_uevent,
 };
 
 static void delete_partition_rcu_cb(struct rcu_head *head)

================================================================
git log -p crypto/blkcipher.c
commit 15aaa954da86024fa1b4e1c26b162fb7be84d2df
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 4 13:28:39 2014 +0800

    crypto: allow blkcipher walks over AEAD data
    
    This adds the function blkcipher_aead_walk_virt_block, which allows the caller
    to use the blkcipher walk API to handle the input and output scatterlists.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/blkcipher.c b/crypto/blkcipher.c
index 46fdab5..0122bec 100644
--- a/crypto/blkcipher.c
+++ b/crypto/blkcipher.c
@@ -357,6 +357,20 @@ int blkcipher_walk_virt_block(struct blkcipher_desc *desc,
 }
 EXPORT_SYMBOL_GPL(blkcipher_walk_virt_block);
 
+int blkcipher_aead_walk_virt_block(struct blkcipher_desc *desc,
+                                  struct blkcipher_walk *walk,
+                                  struct crypto_aead *tfm,
+                                  unsigned int blocksize)
+{
+       walk->flags &= ~BLKCIPHER_WALK_PHYS;
+       walk->walk_blocksize = blocksize;
+       walk->cipher_blocksize = crypto_aead_blocksize(tfm);
+       walk->ivsize = crypto_aead_ivsize(tfm);
+       walk->alignmask = crypto_aead_alignmask(tfm);
+       return blkcipher_walk_first(desc, walk);
+}
+EXPORT_SYMBOL_GPL(blkcipher_aead_walk_virt_block);
+
 static int setkey_unaligned(struct crypto_tfm *tfm, const u8 *key,
                            unsigned int keylen)
 {

commit 29c0c76ffbf9208b6348cef882dd053a6190bb66
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 4 13:28:38 2014 +0800

    crypto: remove direct blkcipher_walk dependency on transform
    
    In order to allow other uses of the blkcipher walk API than the blkcipher
    algos themselves, this patch copies some of the transform data members to the
    walk struct so the transform is only accessed at walk init time.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/crypto/blkcipher.c b/crypto/blkcipher.c
index a79e7e9..46fdab5 100644
--- a/crypto/blkcipher.c
+++ b/crypto/blkcipher.c
@@ -70,14 +70,12 @@ static inline u8 *blkcipher_get_spot(u8 *start, unsigned int len)
        return max(start, end_page);
 }
 
-static inline unsigned int blkcipher_done_slow(struct crypto_blkcipher *tfm,
-                                              struct blkcipher_walk *walk,
+static inline unsigned int blkcipher_done_slow(struct blkcipher_walk *walk,
                                               unsigned int bsize)
 {
        u8 *addr;
-       unsigned int alignmask = crypto_blkcipher_alignmask(tfm);
 
-       addr = (u8 *)ALIGN((unsigned long)walk->buffer, alignmask + 1);
+       addr = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);
        addr = blkcipher_get_spot(addr, bsize);
        scatterwalk_copychunks(addr, &walk->out, bsize, 1);
        return bsize;
@@ -105,7 +103,6 @@ static inline unsigned int blkcipher_done_fast(struct blkcipher_walk *walk,
 int blkcipher_walk_done(struct blkcipher_desc *desc,
                        struct blkcipher_walk *walk, int err)
 {
-       struct crypto_blkcipher *tfm = desc->tfm;
        unsigned int nbytes = 0;
 
        if (likely(err >= 0)) {
@@ -117,7 +114,7 @@ int blkcipher_walk_done(struct blkcipher_desc *desc,
                        err = -EINVAL;
                        goto err;
                } else
-                       n = blkcipher_done_slow(tfm, walk, n);
+                       n = blkcipher_done_slow(walk, n);
 
                nbytes = walk->total - n;
                err = 0;
@@ -136,7 +133,7 @@ err:
        }
 
        if (walk->iv != desc->info)
-               memcpy(desc->info, walk->iv, crypto_blkcipher_ivsize(tfm));
+               memcpy(desc->info, walk->iv, walk->ivsize);
        if (walk->buffer != walk->page)
                kfree(walk->buffer);
        if (walk->page)
@@ -226,22 +223,20 @@ static inline int blkcipher_next_fast(struct blkcipher_desc *desc,
 static int blkcipher_walk_next(struct blkcipher_desc *desc,
                               struct blkcipher_walk *walk)
 {
-       struct crypto_blkcipher *tfm = desc->tfm;
-       unsigned int alignmask = crypto_blkcipher_alignmask(tfm);
        unsigned int bsize;
        unsigned int n;
        int err;
 
        n = walk->total;
-       if (unlikely(n < crypto_blkcipher_blocksize(tfm))) {
+       if (unlikely(n < walk->cipher_blocksize)) {
                desc->flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;
                return blkcipher_walk_done(desc, walk, -EINVAL);
        }
 
        walk->flags &= ~(BLKCIPHER_WALK_SLOW | BLKCIPHER_WALK_COPY |
                         BLKCIPHER_WALK_DIFF);
-       if (!scatterwalk_aligned(&walk->in, alignmask) ||
-           !scatterwalk_aligned(&walk->out, alignmask)) {
+       if (!scatterwalk_aligned(&walk->in, walk->alignmask) ||
+           !scatterwalk_aligned(&walk->out, walk->alignmask)) {
                walk->flags |= BLKCIPHER_WALK_COPY;
                if (!walk->page) {
                        walk->page = (void *)__get_free_page(GFP_ATOMIC);
@@ -250,12 +245,12 @@ static int blkcipher_walk_next(struct blkcipher_desc *desc,
                }
        }
 
-       bsize = min(walk->blocksize, n);
+       bsize = min(walk->walk_blocksize, n);
        n = scatterwalk_clamp(&walk->in, n);
        n = scatterwalk_clamp(&walk->out, n);
 
        if (unlikely(n < bsize)) {
-               err = blkcipher_next_slow(desc, walk, bsize, alignmask);
+               err = blkcipher_next_slow(desc, walk, bsize, walk->alignmask);
                goto set_phys_lowmem;
        }
 
@@ -277,28 +272,26 @@ set_phys_lowmem:
        return err;
 }
 
-static inline int blkcipher_copy_iv(struct blkcipher_walk *walk,
-                                   struct crypto_blkcipher *tfm,
-                                   unsigned int alignmask)
+static inline int blkcipher_copy_iv(struct blkcipher_walk *walk)
 {
-       unsigned bs = walk->blocksize;
-       unsigned int ivsize = crypto_blkcipher_ivsize(tfm);
-       unsigned aligned_bs = ALIGN(bs, alignmask + 1);
-       unsigned int size = aligned_bs * 2 + ivsize + max(aligned_bs, ivsize) -
-                           (alignmask + 1);
+       unsigned bs = walk->walk_blocksize;
+       unsigned aligned_bs = ALIGN(bs, walk->alignmask + 1);
+       unsigned int size = aligned_bs * 2 +
+                           walk->ivsize + max(aligned_bs, walk->ivsize) -
+                           (walk->alignmask + 1);
        u8 *iv;
 
-       size += alignmask & ~(crypto_tfm_ctx_alignment() - 1);
+       size += walk->alignmask & ~(crypto_tfm_ctx_alignment() - 1);
        walk->buffer = kmalloc(size, GFP_ATOMIC);
        if (!walk->buffer)
                return -ENOMEM;
 
-       iv = (u8 *)ALIGN((unsigned long)walk->buffer, alignmask + 1);
+       iv = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);
        iv = blkcipher_get_spot(iv, bs) + aligned_bs;
        iv = blkcipher_get_spot(iv, bs) + aligned_bs;
-       iv = blkcipher_get_spot(iv, ivsize);
+       iv = blkcipher_get_spot(iv, walk->ivsize);
 
-       walk->iv = memcpy(iv, walk->iv, ivsize);
+       walk->iv = memcpy(iv, walk->iv, walk->ivsize);
        return 0;
 }
 
@@ -306,7 +299,10 @@ int blkcipher_walk_virt(struct blkcipher_desc *desc,
                        struct blkcipher_walk *walk)
 {
        walk->flags &= ~BLKCIPHER_WALK_PHYS;
-       walk->blocksize = crypto_blkcipher_blocksize(desc->tfm);
+       walk->walk_blocksize = crypto_blkcipher_blocksize(desc->tfm);
+       walk->cipher_blocksize = walk->walk_blocksize;
+       walk->ivsize = crypto_blkcipher_ivsize(desc->tfm);
+       walk->alignmask = crypto_blkcipher_alignmask(desc->tfm);
        return blkcipher_walk_first(desc, walk);
 }
 EXPORT_SYMBOL_GPL(blkcipher_walk_virt);
@@ -315,7 +311,10 @@ int blkcipher_walk_phys(struct blkcipher_desc *desc,
                        struct blkcipher_walk *walk)
 {
                        struct blkcipher_walk *walk)
 {
        walk->flags |= BLKCIPHER_WALK_PHYS;
-       walk->blocksize = crypto_blkcipher_blocksize(desc->tfm);
+       walk->walk_blocksize = crypto_blkcipher_blocksize(desc->tfm);
+       walk->cipher_blocksize = walk->walk_blocksize;
+       walk->ivsize = crypto_blkcipher_ivsize(desc->tfm);
+       walk->alignmask = crypto_blkcipher_alignmask(desc->tfm);
        return blkcipher_walk_first(desc, walk);
 }
 EXPORT_SYMBOL_GPL(blkcipher_walk_phys);
@@ -323,9 +322,6 @@ EXPORT_SYMBOL_GPL(blkcipher_walk_phys);
 static int blkcipher_walk_first(struct blkcipher_desc *desc,
                                struct blkcipher_walk *walk)
 {
-       struct crypto_blkcipher *tfm = desc->tfm;
-       unsigned int alignmask = crypto_blkcipher_alignmask(tfm);
-
        if (WARN_ON_ONCE(in_irq()))
                return -EDEADLK;
 
@@ -335,8 +331,8 @@ static int blkcipher_walk_first(struct blkcipher_desc *desc,
 
        walk->buffer = NULL;
        walk->iv = desc->info;
-       if (unlikely(((unsigned long)walk->iv & alignmask))) {
-               int err = blkcipher_copy_iv(walk, tfm, alignmask);
+       if (unlikely(((unsigned long)walk->iv & walk->alignmask))) {
+               int err = blkcipher_copy_iv(walk);
                if (err)
                        return err;
        }
@@ -353,7 +349,10 @@ int blkcipher_walk_virt_block(struct blkcipher_desc *desc,
                              unsigned int blocksize)
 {
        walk->flags &= ~BLKCIPHER_WALK_PHYS;
-       walk->blocksize = blocksize;
+       walk->walk_blocksize = blocksize;
+       walk->cipher_blocksize = crypto_blkcipher_blocksize(desc->tfm);
+       walk->ivsize = crypto_blkcipher_ivsize(desc->tfm);
+       walk->alignmask = crypto_blkcipher_alignmask(desc->tfm);
        return blkcipher_walk_first(desc, walk);
 }
 EXPORT_SYMBOL_GPL(blkcipher_walk_virt_block);





